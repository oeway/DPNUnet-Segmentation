<docs lang="markdown">
[TODO: write documentation for this plugin.]
</docs>

<config lang="json">
{
  "name": "DPNUnet-docs",
  "type": "window",
  "tags": [],
  "ui": "",
  "version": "0.1.0",
  "cover": "",
  "description": "Documentation for image segmentation.",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.5",
  "env": "",
  "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.6.10/vue.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/marked/0.6.2/marked.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"],
  "dependencies": [],
  "defaults": {"w": 30, "h": 20}
}
</config>


<attachment name="summary">
<br>
<p id="segmentation-summary"> 
  In this documentation we describe a complete workflow to perform segmentation of nuclei with deep learning.
</p>

<figure>
    <img src=" "  class="img-responsive p-centered" alt="nuclei-segmentaton">
    <figcaption>Nuclei segmentation: DAPI (left), segmented nuclei (right).</figcaption>
</figure>

The segmentation itself is perform largely by the winning approach (DPNUnet) of the 2018 Kaggle Data Science Bowl.

* **Description of DPNUnet**: <a onclick="api.utils.openUrl('https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741')">https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741</a>,
Nature Methods, 2018
* **GitHub**: <a onclick="api.utils.openUrl('https://github.com/selimsef/dsb2018_topcoders/')">https://github.com/selimsef/dsb2018_topcoders/</a>

When starting the DPNUnet-Segmentation plugin, you will see a **central launch pad** allowing to select the task to be performed:

<figure>
    <img src="https://www.dropbox.com/s/8komkrwt1tllj6u/dpnunet-launchpad.PNG?dl=1" width="600" class="img-responsive p-centered" alt="segmentation-launchpad">
    <figcaption>Screen shot of DPNUnet-Segmentation launchpad.</figcaption>
</figure>

**Highlights of the workflow**

* Plugin for annotation of your own data and creating image mask suitable for training of DPNUnet
* Train on your own data starting with the pre-trained DPNUnet
* Perform nuclei segmentation on new data and inspect results in interative tool


</attachment> 

<attachment name="deeplearning">
<br>
<p id="deep-learning-primer">
We provide only a quick overview what deep learning is and establish the necessary
terminology to understand this segmentation workflow. For a more detailed introduction, 
we refer to many available online ressources. 
</p>

<h2>What is deep learning</h2>
Deep learning is a machine learning method that uses an input X to predict
an output Y. This prediction is performed with Deep Neural Networks. 
The essential building block of such a neural network is a perceptron 
which accomplishes simple signal processing. These perceptrons are then 
connected into a large mesh network. 
Importantly, such a network has to be trained before prediction can be performed. 

<figure>
    <img src="https://www.dropbox.com/s/hbg3c23gwfedu0p/neural_network.png?dl=1" width="300" class="img-responsive p-centered" alt="anna-palm-reconstruction">
    <figcaption>Basic structure of a neural network.</figcaption>
</figure>

<h2>Train a neural network</h2>
To train a neural network annotated data has to be provided, for instance a DAPI image 
with outlined nuclei. These data are then used by the neural network to learn how
to detect nuclei in new images. 

The training is performed iteratively and progress is monitored with so-called loss functions. 
These functions measure how well the method performs by comparing the input images to the 
generated images by the neural network. The annotated data is split into 
two data-sets: one for the **actual training**, one for **validation**. The validation data is not 
used for training but to monitor how the model generalizes to new data.

When monitoring training progress different distinct phases are usually observed: 
initially the model can neither describe the training and validation data (**underfitting**). 
Eventually the model will describe the training data too well and not generalize 
anymore to new data (**overfitting**). Good training finds the sweet spot between these two 
extreme cases. 

<figure>
    <img src="https://www.dropbox.com/s/inlro9emd2yk1y6/training_validation.png?dl=1" width="400" class="img-responsive p-centered" alt="anna-palm-reconstruction">
    <figcaption>Training and validation loss during training.</figcaption>
</figure>
 
Training a neural network is **computationally expensive** and is best performed on 
**GPUs** (Graphics Processing Unit) or computational clusters. Applying a trained model is less
demanding and can be done on routinely on regular computers, and depending on the implementation
even on mobile devices.  

Applying a neural network to **new data** frequently requires **re-training**. Here, a already trained
model is used as the starting point to perform training on the new data. 

</attachment>

<attachment name="data">
<br>
<p id="data-requirements">
Here we describe how the training images have to be provided: image format, naming scheme, and data organization. 
</p>

## Data organization for training

Training data has to be organized with the following folder structure:
* Traing and validation data has to be stored in two folders **train** and **test**, respectively. 
* Each sample (e.g. a field of view) is stored in a separate subfolder with a unique name.

Images have to be provided as follows:
* **Filenames have to be identical** across these subfolders, e.g. `nuclei.png`
* Image have to be **2D**. 3D images have to be converted to 2D, e.g. with a maximum intensity projection. 
* Images are stored as **RGB PNGs**.

We then provide plugins to annotate images and create masks:
* **Annotations** can be created with a dedicated plugin resulting in file `annotation.json` within each subfolder. More details
are provided in the dedicated section. 
* The network will learn to predict different mask types (`nuclei_masks.png`), which can be created with a dedicated plugin. 

This will result in the following folder structure
```
├─ data_for_training/
│  ├─ train/
│  │  ├─ img1
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_masks.png
│  │  ├─ img2
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_masks.png
│  │  ...
│  ├─ test/
│  │  ├─ img57
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_masks.png
│  │  ├─ img58
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_masks.png
│  │   ...
```

https://www.dropbox.com/sh/8pfim9t21rrheni/AAB-x4Gk44FrSu1_6k9TXCQwa?dl=0

## Data for prediction

**ToDo**
</attachment>

<attachment name="annotation">
<br>
<p id="annotation">
This plugin allows to annote the structures of interest, i.e. nuclei. 
The plugin was designed to read the above describe folder structure, i.e. opening the
folder `data_for_training` will allow to annotate all contained images. 
</p>

## Load data on plugin engine
1. Prepare training data as specified in the section 'Data'.
2. You can drag and drop the folder containing the subfolder 'train' and 'test'  to the plugin engine. 
3. In ImJOy, press `Files`, and `Open Engine File`.
4. If ImJoy is connected to multiple plugin engines, select the one where the segmentation plugin is running on.
5. Navigate to the folder where you want to load your data. 
5. Drag & drop the folder containing the training data into this folder.

## Import your images into Annotator plugin
When starting the plugin, it will show a default image. If you want to familiarize yourself
with the annotation features, you can already play around with this image.

In order to load your own data, you have to 
1.  Press on the `File` dropdown menu in the upper right corner
2.  Select `Import Samples`
3.  In the new dialog press `Choose Files` and then `Select Plugin Files`
4.  In the dialog, select the folder with the training data.
5.  ImJoy will display a dialog saying "This will upload ....", confirm. Your
    data **only be "uploaded" to your local browser** but not on an external website.
6.  This will populate the interface with all files in your folder. For each folder,
7.  you see a little icon representing a file being present in the folder. Hoovering over 
    the icon will display its name.
8.  You then have to set a filter on the file-name for the channel which will
    be read into the Annotator plugin. For this you set a name for the channel,
    e.g. `nuclei`, and the identifier of this image, e.g. `nuclei.ong`. Pressing on 
    the icon of file, will populate the filter fields with a suggestion, which can 
    be modified further. Press `Add Channel` to add this channel. You then see this 
    channel as an additional column in the interface. 
8.  Press `Import` to open the Annotator with the specified files.

## Annotate structure of interest
You can then specify your annotation: 
1. From the `Annotation` dropdown menu, you can specify which annotations type you want to
  outline. 
2. By pressing `New Marker` you can specify an new annotation type. You can define
  its name, the color in which it will show, and what outline type.

**ToDo update screeenshot to show segmentation **
<img src="https://www.dropbox.com/s/7x6ro3ldckv7cvb/screenshot-annotation.png?dl=1" width="500" class="img-responsive ..." alt="...">
     
## Navigate your image
To faciliate the annotation, you can
* Zoom
* Pan (with the mouse button pressed)
* Change contrast
* Show multiple channels as an overlay
* Display other annotation types.

## Export annotations
Once you are done, you can export the annotations from the `Exports` dropdown menu
and selecting

**ToDo update to give more details**

## Generate masks
Once your images are annotated, you can generate masks that will be predicted by the network and used 
to segment the nuclei. Two mask types are used in DPNUnet: filled and border.

<div class="container">
  <div class="columns">
    <div class="column col-xs-6">Filled mask<img src="https://www.dropbox.com/s/ggrwbrqvpboy9pf/cells__MASK_fill.png?dl=1" class="img-responsive ..." alt="...">
 </div>
    <div class="column col-xs-6"> Border mask   <img src="https://www.dropbox.com/s/qdn85oj6zlwxthw/cells__MASK_edgeWeight.png?dl=1" class="img-responsive ..." alt="...">
  </div>
</div>

When pressing the button `Generate Masks`, a dialog pops up where you can specify the folder containing your annotated 
training and validation data. Masks will then be generated for all annotations and stored as PNG file named `nuclei_masks.png`
</attachment>

<attachment name="training">
<br>
<p id="training">
Here you can re-train the pre-trained DPNUnet model on your own data. 
</p>

In order to re-train you have to 
1. Press button 'Train with data from engine'
2. This will show a configuration dialog, where you can specify the filename of the input images, e.g. `nuclei.png`, and the target masks, e.g. `nuclei_masks.png`. 
 Further you can specify for how many epochs the network will train, and under which name the model will be saved.
<figure>
    <img src="https://www.dropbox.com/s/f0wqnjs67jriabt/dpnunet-training-config.PNG?dl=1" width="400" class="img-responsive p-centered" alt="training-config">
    <figcaption>Training and validation loss during training.</figcaption>
</figure>
3. Specify the training data, i.e. the folder containing subfolders 'train' and 'valid'.
4. Training progress can be inspected with the dashboard. 

<figure>
    <img src="https://www.dropbox.com/s/inlro9emd2yk1y6/training_validation.png?dl=1" width="400" class="img-responsive p-centered" alt="training-dashboard">
    <figcaption>Monitoring training progress with the dashboard.</figcaption>
</figure>


The **trained model ** are stored as **ToDo: correct names and more info**, and can then be used for prediction.

</attachment>

<attachment name="prediction">

For prediction, you have
1. Press button 'Predict'
2. This will show a configuration dialog, where you can specify the filename of the input images, e.g. `nuclei.png`, and the target masks, e.g. `nuclei_masks.png`. 
<figure>
    <img src="https://www.dropbox.com/s/f0wqnjs67jriabt/dpnunet-predict-config.PNG?dl=1" width="400" class="img-responsive p-centered" alt="training-config">
    <figcaption>Training and validation loss during training.</figcaption>
</figure>
3. Select a folder containg a subfolder testing.


WHich model is used?
How does test data have to be organized? also in individual folders?
</attachment>
<script lang="javascript">
class ImJoyPlugin {
  async setup() {
    // For vue
    this.app = new Vue({
      el: '#app',
      data: {
        active_tab: 'summary'
      },
      methods: {
        activateTab(name){
          this.active_tab = name
        },
        focus(tab_name, section_id){
          if(tab_name && section_id){
            this.active_tab = tab_name
            this.nextTick(()=>{
              document.getElementById(section_id).scrollIntoView()
            })
          }
        }
      }
    })
    api.log('initialized')
  }
  async run(ctx) {
      
      document.getElementById('cont_summary').innerHTML =
      marked(await api.getAttachment('summary'));
      document.getElementById('cont_deeplearning').innerHTML =
      marked(await api.getAttachment('deeplearning'));
      document.getElementById('cont_data').innerHTML =
      marked(await api.getAttachment('data')); 
      document.getElementById('cont_annotation').innerHTML =
      marked(await api.getAttachment('annotation'));
      document.getElementById('cont_training').innerHTML =
      marked(await api.getAttachment('training')); 
      document.getElementById('cont_prediction').innerHTML =
      marked(await api.getAttachment('prediction')); 
      try {
        console.log(ctx.data.tab, ctx.data.section)
        this.app.focus(ctx.data.tab, ctx.data.section)
        this.app.$forceUpdate()
      }
      catch(err) {
      console.log(err)
      }
  }
}
api.export(new ImJoyPlugin())
</script>

<window lang="html">
  <div id="app">
    
    <ul class="tab tab-block">
      <li class="tab-item" v-bind:class="{ active: active_tab=='summary' }">
        <a href="#" @click="active_tab='summary'">Summary</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='deeplearning' }">
         <a href="#" @click="active_tab='deeplearning'">Deep learning</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='data' }">
         <a href="#" @click="active_tab='data'">Data</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='annotation' }">
         <a href="#" @click="active_tab='annotation'">Annotation & masks</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='training' }">
        <a href="#" @click="active_tab='training'">Training</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='prediction' }">
        <a href="#" @click="active_tab='prediction'">Prediction</a>
      </li>
    </ul>

   <!-- This can eventually be replaced by a better organization, e.g. tabs -->
   <div v-show="active_tab=='summary'" class="tab-content" id="cont_summary"></div>
   <div v-show="active_tab=='deeplearning'" class="tab-content" id="cont_deeplearning"></div>
   <div v-show="active_tab=='data'" class="tab-content" id="cont_data"></div>
   <div v-show="active_tab=='annotation'" class="tab-content" id="cont_annotation"></div>
   <div v-show="active_tab=='training'" class="tab-content" id="cont_training"></div>
   <div v-show="active_tab=='prediction'" class="tab-content" id="cont_prediction"></div>

  </div>
</window>

<style lang="css">

.tab-content{
  padding: 10px;
}

figcaption {
    background-color: #fff;
    color: #a1c8f2;
    font: italic smaller sans-serif;
    padding: 3px;
    text-align: center;
}

</style>
