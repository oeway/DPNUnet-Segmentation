<docs lang="markdown">
[TODO: write documentation for this plugin.]
</docs>

<config lang="json">
{
  "name": "DPNUnet-docs",
  "type": "window",
  "tags": [],
  "ui": "",
  "version": "0.1.3",
  "cover": "",
  "description": "Documentation for image segmentation.",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.5",
  "env": "",
  "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.6.10/vue.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/marked/0.6.2/marked.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"],
  "dependencies": [],
  "defaults": {"w": 30, "h": 20}
}
</config>


<attachment name="summary">
<br>
<p id="segmentation-summary"> 
  In this documentation we describe a complete workflow to perform segmentation of nuclei with deep learning.
</p>

<figure>
    <img src=" "  class="img-responsive p-centered" alt="nuclei-segmentaton">
    <figcaption>Nuclei segmentation: DAPI (left), segmented nuclei (right).</figcaption>
</figure>

The segmentation is perform by the winning approach (DPNUnet) of the 2018 Kaggle Data Science Bowl.

* **Description of DPNUnet**: <a onclick="api.utils.openUrl('https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741')">https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741</a>,
Nature Methods, 2018
* **GitHub**: <a onclick="api.utils.openUrl('https://github.com/selimsef/dsb2018_topcoders/')">https://github.com/selimsef/dsb2018_topcoders/</a>

When starting the DPNUnet-Segmentation plugin, you will see a **central launch pad** allowing to select the task to be performed:

<figure>
    <img src="https://www.dropbox.com/s/8komkrwt1tllj6u/dpnunet-launchpad.PNG?dl=1" width="600" class="img-responsive p-centered" alt="segmentation-launchpad">
    <figcaption>Screen shot of DPNUnet-Segmentation launchpad.</figcaption>
</figure>

**Highlights of the workflow**

* Plugin for annotation of your own data and creating image mask suitable for training of DPNUnet
* Train on your own data starting with the pre-trained DPNUnet
* Perform nuclei segmentation on new data and inspect results in interative tool


</attachment> 

<attachment name="deeplearning">
<br>
<p id="deep-learning-primer">
We provide only a quick overview what deep learning is and establish the necessary
terminology to understand this segmentation workflow. For a more detailed introduction, 
we refer to many available online ressources. 
</p>

<h2>What is deep learning</h2>
Deep learning is a machine learning method that uses an input X to predict
an output Y. This prediction is performed with Deep Neural Networks. 
The essential building block of such a neural network is a perceptron 
which accomplishes simple signal processing. These perceptrons are then 
connected into a large mesh network. 
Importantly, such a network has to be trained before prediction can be performed. 

<figure>
    <img src="https://www.dropbox.com/s/hbg3c23gwfedu0p/dpnunet-neural_network.png?dl=1" width="280" class="img-responsive p-centered" alt="anna-palm-reconstruction">
    <figcaption>Basic structure of a neural network.</figcaption>
</figure>

<h2>Train a neural network</h2>
To train a neural network annotated data has to be provided, for instance a DAPI image 
with outlined nuclei. These data are then used by the neural network to learn how
to detect nuclei in new images. 

The training is performed iteratively and progress is monitored with so-called loss functions. 
These functions measure how well the method performs by comparing the input images to the 
generated images by the neural network. The annotated data is split into 
two data-sets: one for the **actual training**, one for **validation**. The validation data is not 
used for training but to monitor how the model generalizes to new data.

When monitoring training progress different distinct phases are usually observed: 
initially the model can neither describe the training and validation data (**underfitting**). 
Eventually the model will describe the training data too well and not generalize 
anymore to new data (**overfitting**). Good training finds the sweet spot between these two 
extreme cases. 

<figure>
    <img src="https://www.dropbox.com/s/inlro9emd2yk1y6/dpnunet-training_validation.png?dl=1" width="400" class="img-responsive p-centered" alt="anna-palm-reconstruction">
    <figcaption>Training and validation loss during training.</figcaption>
</figure>
 
Training a neural network is **computationally expensive** and is best performed on 
**GPUs** (Graphics Processing Unit) or computational clusters. Applying a trained model is less
demanding and can be done on routinely on regular computers, and depending on the implementation
even on mobile devices.  

Applying a neural network to **new data** frequently requires **re-training**. Here, a already trained
model is used as the starting point to perform training on the new data. 

</attachment>

<attachment name="data">
<br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: when using the provided remote engine 'imjoy.pasteur.cloud', 
  you have training data already available at 'imjoy/imjoy-paper/SEGMENTATION-DATA/HeLa_DAPI_v1'. 
</p>
<br>
<p id="data-requirements">
Here we describe how the training images have to be provided: image format, naming scheme, and data organization. 
</p>

Annotated training data with DAPI stained HeLa cells is available <a onclick="api.utils.openUrl('https://www.dropbox.com/sh/25zv7itq6kfjaje/AABp2npWGuzEQNGuHu7DddD2a?dl=0')">**here**.</a>

## Data organization for training

Training data has to be organized with the following **folder structure**:
* Data for traning and validation is stored in a folder 'train', data for testing the prediction is stored in folder 'test'. 
* Each sample (e.g. a field of view) is stored in a separate subfolder with a unique name.

Images have to be provided in the following **format**:
* **Filenames have to be identical** across these subfolders, e.g. `nuclei.png`
* Image have to be **2D**. 3D images have to be converted to 2D, e.g. with a maximum intensity projection. 
* Images are stored as **RGB PNGs**.

We then provide plugins to annotate images ('`annotation.json`) and create masks (`nuclei_border_mask.png`). More 
details can be found in the dedicate section. The network will then learn to predict these masks from 
the input images.  

This will result in the following folder structure
```
├─ data_for_training/
│  ├─ train/
│  │  ├─ img1
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_border_mask.png
│  │  ├─ img2
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_border_mask.png
│  │  ...
│  ├─ test/
│  │  ├─ img57
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_border_mask.png
│  │  ├─ img58
│  │  │  ├─ annotation.json
│  │  │  ├─ nuclei.png
│  │  │  ├─ nuclei_border_mask.png
│  │   ...
```

## Data for prediction
**ToDo**
</attachment>

<attachment name="annotation">
<br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: when using the provided remote engine 'imjoy.pasteur.cloud', 
  annotated training data is already available at 'imjoy/imjoy-paper/SEGMENTATION-DATA/HeLa_DAPI_v1'. You can
  open these data in the Annotator to inspect the annotations, or directly create masks as described below.
</p>
<br>
<p id="annotation">
This plugin allows to annote the nuclei in the images. The plugin was designed to read the 
above describe folder structure, i.e. opening the folder `data_for_training` will allow to 
annotate all contained images. 
</p>


## Annotator plugin
This plugin allows to annotate different structures of interests in mult-channel images.
You can load an entire data-set containing files in subfolders into the Annotator and annotate these files.
Annotations are stored in geojson format and can be either be loaded on a plugin engine for mask 
generation and training, or retrieved for later use. 

If you want to familiarize yourself with the Annotoation tool, you can simply start it by pressing `Annotations`.
It will open with a default image, and you can already familiarize yourself with some of the features
* Zooming and panning.
* Change contrast
* Show multiple channels as an overlay
* Annotate different structurs, and toggle display of certain annotations.

<figure>
    <img src="https://www.dropbox.com/s/71ujzlf8rfzw7t8/dpnunet-annotation.png?dl=1" width="500" class="img-responsive p-centered" alt="segmentation-launchpad">
    <figcaption>Image annotation in ImJoy.</figcaption>
</figure>

## Import your images into Annotator plugin
1. Prepare training data as specified in the section 'Data'.
0. These data, i.e. the folder containing the subfolder 'train' and 'test', then have to copied 
  to plugin engine, where the segmentation plugin is running. See FAQ for more details for how to load data on an engine. 
0. To read the data into the Annotator, follow these steps
      1.  Press on the `File` dropdown menu in the upper right corner of the ImJoy interface and select `Import Samples`.
      0.  This will open a file import dialog. In the upper left corner select `Select Plugin Files` and select
          the folder with the training data.
      0.  This will populate the interface with all files in the folder. An icon 
          is shown for each present file, hoovering over this icon will display its name.
      0.  You then have to set a **filter on the file-name** to determine which image(s) 
          will be read into the Annotator. For this you set a name for the channel,
          e.g. `nuclei`, and the identifier of this image, e.g. `nuclei.png`. Note that 
          pressing on an icon of file, will populate the filter fields with a suggestion, which can 
          be modified further. Press `Add Channel` to add this channel. You then see this 
          channel as an additional column in the interface, and for each channel if such a file was found.
      0.  Press `Import` to open the Annotator with the specified files.

<figure>
    <img src="https://www.dropbox.com/s/tket24vfukarmzn/dpnunet-annotator-load-data.gif?dl=1" class="img-responsive p-centered" alt="segmentation-launchpad">
    <figcaption>Image annotation in ImJoy.</figcaption>
</figure>

## Annotate structure of interest
You can then specify your annotation: 
1. From the `Annotation` dropdown menu, you can specify which annotations type you want to
  outline. 
2. By pressing `New Marker` you can specify an new annotation type. You can define
  its name, the color in which it will show, and what outline type ('polygon' for nuclei).

## Export annotations
Once you are done, you can export the annotations from the `Exports` dropdown menu
and selecting

**ToDo update to give more details**

## Generate masks
The DPNUnet learns to predict two mask from the input image: a filled mask, and a mask for areas where nuclei touch (so called 'border')
You can generate these masks from the annotated data by pressing `Generate Masks` and select the folder containing the annotated data.  
The masks will be stored as `nuclei_border_mask.png` for each annotated image. 

<div class="container">
  <div class="columns">
    <div class="column col-xs-4">Annotation<img src="https://www.dropbox.com/s/9ayhcbpevzv757b/dpnunet-nuclei-annotation.png?dl=1" class="img-responsive ..." alt="...">
 </div>
  <div class="column col-xs-4"> Filled mask <img src="https://www.dropbox.com/s/ggrwbrqvpboy9pf/dpnunet-nuclei-mask-fill.png?dl=1" class="img-responsive ..." alt="...">
  </div>
    <div class="column col-xs-4"> Border mask <img src="https://www.dropbox.com/s/z2mkpyw2mgi9ffx/dpnunet-nuclei-mask-border.png?dl=1" class="img-responsive ..." alt="...">
  </div>
</div>

</attachment>

<attachment name="training">
 <br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: when using the provided remote engine 'imjoy.pasteur.cloud', 
  annotated training data with masks is  available at 'imjoy/imjoy-paper/SEGMENTATION-DATA/HeLa_DAPI_v1'. 
  You can use these data to perform training.  
</p>
<br>

Here you can re-train the pre-trained DPNUnet model on your own data by following these steps
1. Press button 'Train with data from engine'
2. This will show a configuration dialog, where you can specify the filename of the input images, e.g. `nuclei.png`, and the target masks, e.g. `nuclei_border_mask.png`. 
 Further you can specify for how many epochs the network will train, and under which name the model will be saved.
<figure>
    <img src="https://www.dropbox.com/s/f0wqnjs67jriabt/dpnunet-training-config.PNG?dl=1" width="300" class="img-responsive p-centered" alt="training-config">
    <figcaption>Training and validation loss during training.</figcaption>
</figure>
3. Specify the training data, i.e. the folder containing subfolders 'train' and 'valid'.
4. Training progress can be inspected with the dashboard. 

<figure>
    <img src="https://www.dropbox.com/s/inlro9emd2yk1y6/training_validation.png?dl=1" width="400" class="img-responsive p-centered" alt="training-dashboard">
    <figcaption>Monitoring training progress with the dashboard.</figcaption>
</figure>

The **trained model** are stored as **ToDo: correct names and more info**, and can then be used for prediction.

</attachment>

<attachment name="prediction">
<br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: when using the provided remote engine 'imjoy.pasteur.cloud', 
  the pre-trained model is available at 'imjoy/imjoy-paper/SEGMENTATION-MODELS/pretrained-model-dpn-softmax/fold0_best.pth'.
</p>

## Model for prediction
You can already use a model that you re-trained on your own data (see section 'Training') or use
a pre-trained model which is available <a onclick="api.utils.openUrl('https://www.dropbox.com/s/tufkbdta441qqtz/dpn_softmax_f0_fold0_best.pth?dl=0')">**here**.</a>
To use the pre-trained model, you have to first download it and the load it into the plugin engine as described in the FAQ.

## Perform prediction
1. Press button `Predict`. 
0. This will show a configuration dialog, where you can specify the filename of the input images, e.g. `nuclei.png`, and the target masks, e.g. `nuclei_masks.png`. 
<figure>
    <img src="https://www.dropbox.com/s/f0wqnjs67jriabt/dpnunet-predict-config.PNG?dl=1" width="400" class="img-responsive p-centered" alt="training-config">
    <figcaption>Configuring prediction with DPNUnet.</figcaption>
</figure>
0. Select a folder containing a subfolder 'test'.
0. You will then be asked to specify the approximate size of the nuclei in the image (e.g. 200 for the provided example data).
0. Predicted data will then be shown in a dashboard, where you can use the slider to navigate the different samples.
<figure>
    <img src="https://www.dropbox.com/s/a2gphxu25p8e4yl/dpnunet-prediction-dashboard.png?dl=1" width="400" class="img-responsive p-centered" alt="training-config">
    <figcaption>Inspecting the prediction results.</figcaption>
</figure>
0. The predicted masks will be saved as `nuclei_border_mask_output.png` in each sample folder.
0. To obtain the actual nuclei, a watershed segmentation is performed as follows: the predicted border is 
 mask is substracted from the predicted filled mask. The resulting image is used as a seed for a watershed
 segmentation on the predicted filled mask. This image is saved as `nuclei_border_mask_output_watershed.png`.
## 

**Open questions**
* Which model is used?

</attachment>

<attachment name="faq">
Here we provide answer to frequenty asked questions and encountered problems.

## How can I load data or a model into the plugin engine?
You can simply drag and drop a folder containing either training data or model:
1. In ImJoy, press `Files`, and `Open Engine File`.
2. If ImJoy is connected to multiple plugin engines, select the one where the segmentation plugin is running in the upper part of the interface.
3. Navigate to the folder where you want to store your data. 
4. Drag & drop the folder from Finder (MacOS) or Explorer (WIN) into the ImJoy file dialog.

<figure>
    <img src="https://www.dropbox.com/s/oqofqh7fbnjgb0v/dpnunet-upload.gif?dl=1" width="700" class="img-responsive p-centered" alt="load-model-into-engine">
    <figcaption>Load data on a plugin engine.</figcaption>
</figure>

</attachment>

<script lang="javascript">
class ImJoyPlugin {
  async setup() {
    // For vue
    this.app = new Vue({
      el: '#app',
      data: {
        active_tab: 'summary'
      },
      methods: {
        activateTab(name){
          this.active_tab = name
        },
        focus(tab_name, section_id){
          if(tab_name && section_id){
            this.active_tab = tab_name
            this.nextTick(()=>{
              document.getElementById(section_id).scrollIntoView()
            })
          }
        }
      }
    })
    api.log('initialized')
  }
  async run(ctx) {
      
      document.getElementById('cont_summary').innerHTML =
      marked(await api.getAttachment('summary'));
      document.getElementById('cont_deeplearning').innerHTML =
      marked(await api.getAttachment('deeplearning'));
      document.getElementById('cont_data').innerHTML =
      marked(await api.getAttachment('data')); 
      document.getElementById('cont_annotation').innerHTML =
      marked(await api.getAttachment('annotation'));
      document.getElementById('cont_training').innerHTML =
      marked(await api.getAttachment('training')); 
      document.getElementById('cont_prediction').innerHTML =
      marked(await api.getAttachment('prediction')); 
      document.getElementById('cont_faq').innerHTML =
      marked(await api.getAttachment('faq')); 
      try {
        console.log(ctx.data.tab, ctx.data.section)
        this.app.focus(ctx.data.tab, ctx.data.section)
        this.app.$forceUpdate()
      }
      catch(err) {
      console.log(err)
      }
  }
}
api.export(new ImJoyPlugin())
</script>

<window lang="html">
  <div id="app">
    
    <ul class="tab tab-block">
      <li class="tab-item" v-bind:class="{ active: active_tab=='summary' }">
        <a href="#" @click="active_tab='summary'">Summary</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='deeplearning' }">
         <a href="#" @click="active_tab='deeplearning'">Deep learning</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='data' }">
         <a href="#" @click="active_tab='data'">Data</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='annotation' }">
         <a href="#" @click="active_tab='annotation'">Annotation & masks</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='training' }">
        <a href="#" @click="active_tab='training'">Training</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='prediction' }">
        <a href="#" @click="active_tab='prediction'">Prediction</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='faq' }">
        <a href="#" @click="active_tab='faq'">FAQ</a>
      </li>
    </ul>

   <!-- This can eventually be replaced by a better organization, e.g. tabs -->
   <div v-show="active_tab=='summary'" class="tab-content" id="cont_summary"></div>
   <div v-show="active_tab=='deeplearning'" class="tab-content" id="cont_deeplearning"></div>
   <div v-show="active_tab=='data'" class="tab-content" id="cont_data"></div>
   <div v-show="active_tab=='annotation'" class="tab-content" id="cont_annotation"></div>
   <div v-show="active_tab=='training'" class="tab-content" id="cont_training"></div>
   <div v-show="active_tab=='prediction'" class="tab-content" id="cont_prediction"></div>
   <div v-show="active_tab=='faq'" class="tab-content" id="cont_faq"></div>
  </div>
</window>

<style lang="css">

.tab-content{
  padding: 10px;
}

figcaption {
    background-color: #fff;
    color: #a1c8f2;
    font: italic smaller sans-serif;
    padding: 3px;
    text-align: center;
}

</style>
